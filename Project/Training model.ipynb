{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer # tokenizing\n",
    "from nltk.corpus import stopwords  # list of stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white space -- very naive, fist pass \n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(title, rmv_stop_words=False):\n",
    "    tokens = tokenizer.tokenize(title.lower())     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "    normalized = [lemma.lemmatize(token) for token in tokens] # lemma\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'posts_titles_30M.txt'\n",
    "corpus = []\n",
    "with open(path+file, 'r') as f:\n",
    "    for line in f:\n",
    "        corpus.append(clean(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)\n",
    "print(corpus[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = gensim.corpora.Dictionary(corpus)\n",
    "#print(len(dictionary))\n",
    "#dictionary.filter_extremes(no_below=2)#no_above = .5) # can play with this\n",
    "#len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_nostop = gensim.models.word2vec.Word2Vec(corpus_nostop, sg=1, size=200, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_doc='30M'\n",
    "sg=1\n",
    "size=200\n",
    "window=5\n",
    "min_count=2\n",
    "model_full = gensim.models.word2vec.Word2Vec(corpus, sg=sg, size=size, window=window, min_count=min_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on log report, some considerations:\n",
    " - NEED MORE DATA\n",
    " - consider feeding sentences without removing stop words\n",
    " - based on available data and number of unique words and word frequencies, adjust min_count\n",
    " - review more notes on logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_{}_{}_{}_{}_{}'.format(num_doc,sg,size,window,min_count)\n",
    "model_full.save(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brief tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nostop.most_similar(['iterate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('animal', 0.5973125696182251),\n",
       " ('thisisacat', 0.5312849283218384),\n",
       " ('horse', 0.5259320735931396),\n",
       " ('strlist', 0.504112184047699),\n",
       " ('x18k', 0.4975487291812897),\n",
       " ('distingushing', 0.4932793974876404),\n",
       " ('var_a', 0.49075835943222046),\n",
       " ('meatier', 0.48547616600990295),\n",
       " ('invent', 0.48366284370422363),\n",
       " ('parentdetail', 0.4828881621360779)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_full.most_similar(['iterate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**synonyms work pretty well!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nostop.predict_output_word(['iterate','dataframe','rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_full.predict_output_word(['iterate','dataframe','rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"How to find all questions that were duplicates of another question\".split()\n",
    "model_full.predict_output_word(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
