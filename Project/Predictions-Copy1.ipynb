{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#import logging\n",
    "\n",
    "#import string\n",
    "from nltk.tokenize import RegexpTokenizer # tokenizing\n",
    "from nltk.corpus import stopwords  # list of stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # lemmatizer\n",
    "\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model\n",
    "\n",
    "Choose one option from below.  The first 2 are the same model, which can be used to calculate a probability score for a phrase. The first is just the word vectors (no hidden layer weights). The second is the full model (can calculate score).\n",
    "\n",
    "The last file is the negative sampling model, which always for the 'predict_output_word' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load keyed vector file (model with softmax, CBOW, and no negative sampling)\n",
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_50M_sg0_sz250_win5_min3_hs1_neg0_kv'\n",
    "# CBOW, window=250, min_count=3, hierarchical softmax, \n",
    "del model_full\n",
    "model_kv = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full model with softmax, CBOW, and no negative sampling\n",
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_50M_sg0_sz250_win5_min3_hs1_neg0'\n",
    "model= gensim.models.word2vec.Word2Vec.load(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load full model with skipgram and negative sampling\n",
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_50M_1_250_5_3'\n",
    "model_ns = gensim.models.word2vec.Word2Vec.load(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white spice\n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(title, rmv_stop_words=False):\n",
    "    tokens = tokenizer.tokenize(title.lower())     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "    normalized = [lemma.lemmatize(token) for token in tokens] # lemma\n",
    "    return normalized\n",
    "\n",
    "def predict_similar(query, model, rmv_stop_words=False):\n",
    "    l = []\n",
    "    q = clean(query, rmv_stop_words=rmv_stop_words)\n",
    "    print('Original query: {}\\n'.format(query))\n",
    "    for word in q:\n",
    "        missing = q[:]\n",
    "        ind = q.index(word)\n",
    "        missing.remove(word)\n",
    "        for syn in model.most_similar([word],topn=3):\n",
    "            full = missing[:]\n",
    "            full.insert(ind,syn[0])\n",
    "            l.append(' '.join(full))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" These generate alternative queries and score them and filter them \"\"\"\n",
    "def generate_alternatives(query, n, model, rmv_stop_words=False):\n",
    "    syns = get_similar(query, n, model, rmv_stop_words) # synonyms\n",
    "    combs = get_combinations(syns) # combinations\n",
    "    probs = [model.score([sug])[0] for sug in combs] # probabilities\n",
    "    preds_probs =[(p,q) for p,q in zip(probs,combs)] # combine with queries\n",
    "    q_score = model.score([clean(query)])[0] # score for original query\n",
    "    sd = get_sd(preds_probs)\n",
    "    preds_1sd = [(x,y) for x,y in preds_probs if np.abs(x-q_score)<=sd] # keep just those within 1 sd\n",
    "    preds_1sd.sort(reverse=True)\n",
    "    print(\"original query: {}\".format(query))\n",
    "    print(\"score: {}\".format(q_score))\n",
    "    print(\"sd of all results: {}\".format(sd))\n",
    "    print(\"number of results within 1 SD of original query score: {}\".format(len(preds_1sd)))\n",
    "    return preds_1sd\n",
    "\n",
    "def get_similar(query, n, model, rmv_stop_words):\n",
    "    q = clean(query, rmv_stop_words=rmv_stop_words)\n",
    "    # turn each word  of query into its own list\n",
    "    d = [[x] for x in q]\n",
    "    for x in d:\n",
    "        # for each word in original query, add topn similar words to list\n",
    "        x.extend([syn for syn,_ in model.most_similar(x[0],topn=n)])\n",
    "    return d\n",
    "\n",
    "def get_combinations(l):\n",
    "    combs = [x for x in product(*l)]\n",
    "    return combs\n",
    "\n",
    "def get_sd(tups):\n",
    "    vals = [x for x,_ in tups]\n",
    "    return np.std(vals)\n",
    "\n",
    "def clean_preds(pred_scores, topn=3):\n",
    "    clean = []\n",
    "    i = 0\n",
    "    for score,query in pred_scores:\n",
    "        i+=1\n",
    "        if i > topn: break\n",
    "        clean.append((score, ' '.join(query)))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"These query stack overflow and return and parse the serach results\"\"\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "            'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            'Accept_Encoding': 'gzip, deflate, sdch, br', 'Accept_Language':'en-US,en;q=0.8',\n",
    "            'Connection': 'keep-alive'}\n",
    "\n",
    "def get_query_results(query):\n",
    "    url = 'https://stackoverflow.com/search?q='+'+'.join(query)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    x = parse_results(soup)\n",
    "    return x\n",
    "    \n",
    "def parse_results(soup):\n",
    "    l = []\n",
    "    results = soup.find_all(\"div\", class_=\"question-summary search-result\")\n",
    "    for result in results:\n",
    "        votes = [v.get_text() for v in result.find_all(\"strong\")]\n",
    "        link = result.find(\"div\", class_=\"result-link\").find('a')\n",
    "        query = link.attrs['title']\n",
    "        url = link.attrs['href']\n",
    "        votes.extend([query,url])\n",
    "        l.append(tuple(votes))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-11.837975, 'looping through panda dataframe'),\n",
       " (-12.610452, 'loop through panda dataframe'),\n",
       " (-12.617397, 'iterating over panda dataframe'),\n",
       " (-12.801682, 'iterating through panda dataframe')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original query: iterate over pandas dataframe\n",
      "score: -11.328330993652344\n",
      "sd of all results: 10.13688850402832\n",
      "number of results within 1 SD of original query score: 15\n"
     ]
    }
   ],
   "source": [
    "orig_query = 'iterate over pandas dataframe'\n",
    "orig_query_vec = clean(orig_query)\n",
    "preds = generate_alternatives(orig_query, 3, model)\n",
    "best_preds = clean_preds(preds, 4)\n",
    "# remove any top queries identical to original (so i can make valid comparison)\n",
    "best_preds2 = [(s,q) for s,q in best_preds if q!=' '.join(orig_query_vec)]\n",
    "orig = (model.score([orig_query_vec])[0], ' '.join(orig_query_vec))\n",
    "#if [query==q for _,q in best_preds]):\n",
    "#    best_preds.insert(0,(model.score([clean(query)])[0], ' '.join(clean(query))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/search?q=looping+through+panda+dataframe\n",
      "https://stackoverflow.com/search?q=iterate+through+panda+dataframe\n",
      "https://stackoverflow.com/search?q=iterating+over+panda+dataframe\n",
      "https://stackoverflow.com/search?q=iterate+over+panda+dataframe\n"
     ]
    }
   ],
   "source": [
    "# query each suggestion and scrape relevent metrics\n",
    "sug_q_results ={}\n",
    "for _,q in best_preds2:\n",
    "    sug_q_results[q] = get_query_results(clean(q))\n",
    "    time.sleep(5)\n",
    "orig_q_results = {}\n",
    "orig_q_results[orig_query] = get_query_results(orig_query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['looping through panda dataframe', 'iterate through panda dataframe', 'iterating over panda dataframe']\n"
     ]
    }
   ],
   "source": [
    "# just for convenience do to manual checks later\n",
    "queries = list(sug_q_results)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef make_summaries(key, dic):\\n    d = defaultdict(dict)\\n    votes = 0\\n    answers = 0\\n    for vote,answer,query,_ in dic[list(original)[0]]:\\n        if query in shared:\\n            continue\\n            votes += int(vote)\\n            answers += int(answer)\\n        d[key]['votes'] = votes\\n        d[key]['answers'] = answers\\n        d[key]['unique'] = orig_unique\\n        d[key]['unique_perc'] = len(orig_unique)*1.0/len(orig_items)\\n        \""
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare(suggestions, original):\n",
    "    # get result titles from original in a list\n",
    "    orig_items = [title for _,_,title,_ in original[list(original)[0]]]\n",
    "    summaries = defaultdict(dict)\n",
    "    orig_summaries = defaultdict(dict)\n",
    "    for k in suggestions:\n",
    "        # isolate titles for suggested results\n",
    "        sug_items = [title for _,_,title,_ in suggestions[k]]\n",
    "        \n",
    "        # identify which results are shared with the original search results\n",
    "        shared = set(orig_items) & set(sug_items)\n",
    "        orig_unique = list(set(orig_items).difference(set(sug_items)))\n",
    "        sug_unique = list(set(sug_items).difference(set(orig_items)))\n",
    "        concordance = len(shared)*1.0/len(sug_items)\n",
    "        \n",
    "        votes = 0\n",
    "        answers = 0\n",
    "        for vote,answer,query,_ in original[list(original)[0]]:\n",
    "            if query in shared:\n",
    "                continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "        orig_summaries[k]['unique'] = len(orig_unique)\n",
    "        try: \n",
    "            orig_summaries[k]['avg_unique_votes'] = votes*1.0/orig_summaries[k]['unique']\n",
    "            orig_summaries[k]['avg_unique_answers'] = answers*1.0/orig_summaries[k]['unique']\n",
    "        except ZeroDivisionError:\n",
    "            orig_summaries[k]['avg_unique_votes'] = 0\n",
    "            orig_summaries[k]['avg_unique_answers'] = 0\n",
    "        orig_summaries[k]['titles'] = orig_unique\n",
    "        orig_summaries[k]['concordance'] = concordance\n",
    "        \n",
    "        # calculate sum of votes and answers for all unique search results in suggestion\n",
    "        votes = 0\n",
    "        answers = 0\n",
    "        #distances = []\n",
    "        for vote,answer,query,url in suggestions[k]:\n",
    "            if query in shared:\n",
    "                continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "            #distances.append ()# cosine distance fucntion here)\n",
    "            # concordance\n",
    "        summaries[k]['unique'] = len(sug_unique)\n",
    "        try: \n",
    "            summaries[k]['avg_unique_votes'] = votes*1.0/summaries[k]['unique']\n",
    "            summaries[k]['avg_unique_answers'] = answers*1.0/summaries[k]['unique']\n",
    "        except ZeroDivisionError:\n",
    "            summaries[k]['avg_unique_votes'] = 0\n",
    "            summaries[k]['avg_unique_answers'] = 0\n",
    "        summaries[k]['unique'] = len(sug_unique)\n",
    "        summaries[k]['titles'] = sug_unique\n",
    "        summaries[k]['concordance'] = concordance\n",
    "    return summaries, orig_summaries\n",
    "\n",
    "\"\"\"\n",
    "def make_summaries(key, dic):\n",
    "    d = defaultdict(dict)\n",
    "    votes = 0\n",
    "    answers = 0\n",
    "    for vote,answer,query,_ in dic[list(original)[0]]:\n",
    "        if query in shared:\n",
    "            continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "        d[key]['votes'] = votes\n",
    "        d[key]['answers'] = answers\n",
    "        d[key]['unique'] = orig_unique\n",
    "        d[key]['unique_perc'] = len(orig_unique)*1.0/len(orig_items)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries,orig_summaries = compare(sug_q_results, orig_q_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dif_scores = []\n",
    "votes_dif_scores = []\n",
    "count_unqiue_items = []\n",
    "for k in summaries:\n",
    "    answers_dif_scores.append(summaries[k]['avg_unique_answers'] - orig_summaries[k]['avg_unique_answers'])\n",
    "    votes_dif_scores.append(summaries[k]['avg_unique_votes'] - orig_summaries[k]['avg_unique_votes'])\n",
    "    count_unqiue_items.append(summaries[k]['unique'])\n",
    "avg_answers_dif_score = np.mean(answers_dif_scores)\n",
    "avg_votes_dif_score = np.mean(votes_dif_scores)\n",
    "avg_unique_items = np.mean(count_unqiue_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = defaultdict(dict)\n",
    "\n",
    "# add results to validation results dictionary\n",
    "orig_prob = model.score([orig_query_vec])[0]\n",
    "validation_results[orig_query]['avg_unique_items'] = avg_unique_items\n",
    "validation_results[orig_query]['avg_answers_dif_score'] = avg_answers_dif_score\n",
    "validation_results[orig_query]['avg_votes_dif_score'] = avg_votes_dif_score\n",
    "validation_results[orig_query]['avg_prob_dif_score'] = np.mean([np.exp(prob)-np.exp(orig_prob) for prob,q in best_preds2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'iterate over pandas dataframe': {'avg_answers_dif_score': -0.61666666666666659,\n",
       "              'avg_prob_dif_score': -6.3273023e-06,\n",
       "              'avg_unique_items': 11.333333333333334,\n",
       "              'avg_votes_dif_score': -11.927777777777779}})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(suggestions, original):  \n",
    "    summaries = defaultdict(dict)\n",
    "    for k in suggestions:\n",
    "        votes = 0\n",
    "        answers = 0\n",
    "        #distances = []\n",
    "        for vote,answer,query,url in d[k]:\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "            #distances.append ()# cosine distance fucntion here)\n",
    "            # concordance\n",
    "        summaries[k]['votes'] = votes\n",
    "        summaries[k]['answers'] = answers\n",
    "    return summaries\n",
    "    #summaries[d]['distances'] = # calculate average distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_q_summaries = summarize_results(sug_q_results)\n",
    "orig_q_summary = summarize_results(orig_q_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': 36, 'votes': 594}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_q_summary[orig_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'votes': 518, 'answers': 29}\n",
      "{'votes': 136, 'answers': 19}\n",
      "{'votes': 593, 'answers': 35}\n"
     ]
    }
   ],
   "source": [
    "for k in sug_q_summaries:\n",
    "    print(sug_q_summaries[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**looks pretty good**\n",
    "\n",
    "the highest rated sentences are pretty good substitutes == \n",
    "\n",
    "I think the score method works well for capturing the whole meaning of the sentence because it calculates the probability of the sentence itself --\n",
    "\n",
    "problem is that it doesn't compare it to the original (ie, a suggested query may have higher probability of occuring..ie make more sense ... but it doesn't necessarily mean it conveys the same question/meaning as the original query).... would need something like cosine distance to get similarity.  ideally we want both -- more probable phrasing while maintaining similarity to original query.\n",
    "\n",
    "To me this suggests that this method might be a good choice for determining which sentences to suggest in the first place -- low probability sentences probably shouldn't be shown (except maybe if there is one rare word in the sentence that is intentional)\n",
    "\n",
    "...from here, i could grab the top 10 suggestions (minus the original query), get query results from stack over flow, and calculate a few things:\n",
    "\n",
    "-- amount of overlap with the top 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q)\n",
    "model.score([clean(q)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**\n",
    "you can also get a score for the original word and then show only the suggestions that have a better score ... or within a certain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(q)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploring cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentence, model):\n",
    "    s = 0.0\n",
    "    vec = clean(sentence)\n",
    "    for word in vec:\n",
    "        s += model[word]\n",
    "    return s/len(vec) # using sum or average makes no difference in cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec = sent2vec(q, model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "for sentence in preds:\n",
    "    vecs.append(sent2vec(sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim.models.keyedvectors.KeyedVectors.\n",
    "dist = []\n",
    "for vec in vecs:\n",
    "    dist.append(cosine(q_vec, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cos =[(p,q) for p,q in zip(dist,preds)]\n",
    "preds_cos.sort()\n",
    "preds_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not liking this method too much -- the lowest ranked item (err, largest cosine distance) is actually a pretty good substitue for the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notes**:\n",
    "  - 'iteration' substituations work well, possibly because verb?\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full.mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full.most_similar(['someone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_similar('Someone flagged my question as already answered, but it\\'s not', rmv_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clean('Adding new column to existing DataFrame in Python pandas')\n",
    "b = clean('How can I add a new computed column in a dataframe?') # [duplicate]\n",
    "c = clean('pandas create new column based on values from other columns')\n",
    "d = clean('Add new column in Pandas DataFrame Python')\n",
    "e = clean('Calling an external command in Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_vec(token_list, model):\n",
    "    s = model[token_list[0]]*0\n",
    "    for word in token_list:\n",
    "        s = s + model[word]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [a,b,c]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vec = title_vec(a, model_full)\n",
    "b_vec = title_vec(b, model_full)\n",
    "c_vec = title_vec(c, model_full)\n",
    "d_vec = title_vec(d, model_full)\n",
    "e_vec = title_vec(e, model_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "print(sp.spatial.distance.euclidean(a_vec,b_vec))\n",
    "print(sp.spatial.distance.euclidean(a_vec,c_vec))\n",
    "print(sp.spatial.distance.euclidean(b_vec,c_vec))\n",
    "print(sp.spatial.distance.euclidean(a_vec,e_vec))\n",
    "print(sp.spatial.distance.euclidean(a_vec,d_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at AOL data for possible validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path='/Users/stevenfelix/Downloads/'\n",
    "file='user-ct-test-collection-01.txt'\n",
    "data = pd.read_table(path+file, delimiter='\\t')\n",
    "data.iloc[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.com' in 'rentdirect.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rem = ['.gov', '.com', '.edu', 'www', '.net', 'http', '.org']\n",
    "for x in rem:\n",
    "    data = data[[(x not in str(v)) for _,v in data.Query.iteritems()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[1:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
