{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#import logging\n",
    "\n",
    "#import string\n",
    "from nltk.tokenize import RegexpTokenizer # tokenizing\n",
    "from nltk.corpus import stopwords  # list of stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # lemmatizer\n",
    "\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import html\n",
    "\n",
    "from collections import defaultdict\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model\n",
    "\n",
    "Choose one option from below.  The first 2 are the same model, which can be used to calculate a probability score for a phrase. The first is just the word vectors (no hidden layer weights). The second is the full model (can calculate score).\n",
    "\n",
    "The last file is the negative sampling model, which always for the 'predict_output_word' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full model with softmax, CBOW, and no negative sampling\n",
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_50M_sg0_sz250_win5_min3_hs1_neg0'\n",
    "model= gensim.models.word2vec.Word2Vec.load(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"take a single phrase as a string, and preprocess, return tokens\"\"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white spice\n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(title, rmv_stop_words=False):\n",
    "    tokens = tokenizer.tokenize(title.lower())     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "    normalized = [lemma.lemmatize(token) for token in tokens] # lemma\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "contractions = re.compile(r\"'|-|\\\"\")\n",
    "# all non alphanumeric\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "# single character removal\n",
    "singles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "# tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white spice\n",
    "# stop words\n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text, rmv_stop_words=True, return_tokens=False): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = symbols.sub(r' \\1 ', text)\n",
    "    text = singles.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    tokens = tokenizer.tokenize(text)     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "        text = ' '.join(tokens)\n",
    "    if return_tokens:\n",
    "        return tokens\n",
    "    return text\n",
    "\n",
    "# sentence splitter\n",
    "#alteos = re.compile(r'([!\\?])')\n",
    "#def sentences(l):\n",
    "#    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n",
    "#    return l.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" These generate alternative queries and score them and filter them \"\"\"\n",
    "def generate_alternatives(query, n, model, rmv_stop_words=False):\n",
    "    print('getting similar words')\n",
    "    syns = get_similar(query, n, model, rmv_stop_words) # synonyms\n",
    "    print('making combinations')\n",
    "    combs = get_combinations(syns) # combinations\n",
    "    # generatings probaiblity scores\n",
    "    probs = [model.score([sug])[0] for sug in combs] # probabilities\n",
    "    preds_probs =[(p,q) for p,q in zip(probs,combs)] # combine with queries\n",
    "    q_score = model.score([tokenizer.tokenize(query)])[0] # score for original query\n",
    "    sd = get_sd(preds_probs)\n",
    "    preds_1sd = [(x,y) for x,y in preds_probs if np.abs(x-q_score)<=sd] # keep just those within 1 sd\n",
    "    preds_1sd.sort(reverse=True)\n",
    "    #print(\"original query: {}\".format(query))\n",
    "    #print(\"score: {}\".format(q_score))\n",
    "    #print(\"sd of all results: {}\".format(sd))\n",
    "    #print(\"number of results within 1 SD of original query score: {}\".format(len(preds_1sd)))\n",
    "    return preds_1sd\n",
    "\n",
    "def get_similar(query, n, model, rmv_stop_words):\n",
    "    q = clean(query, rmv_stop_words=rmv_stop_words, return_tokens=True)\n",
    "    # turn each word  of query into its own list\n",
    "    d = [[x] for x in q]\n",
    "    for x in d:\n",
    "        # for each word in original query, add topn similar words to list\n",
    "        x.extend([syn for syn,_ in model.most_similar(x[0],topn=n)])\n",
    "    return d\n",
    "\n",
    "def get_combinations(l):\n",
    "    combs = [x for x in product(*l)]\n",
    "    return combs\n",
    "\n",
    "def get_sd(tups):\n",
    "    vals = [x for x,_ in tups]\n",
    "    return np.std(vals)\n",
    "\n",
    "def clean_preds(pred_scores, topn=3):\n",
    "    clean = []\n",
    "    i = 0\n",
    "    for score,query in pred_scores:\n",
    "        i+=1\n",
    "        if i > topn: break\n",
    "        clean.append((score, ' '.join(query)))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"These query stack overflow and return and parse the serach results for validation\"\"\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "            'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            'Accept_Encoding': 'gzip, deflate, sdch, br', 'Accept_Language':'en-US,en;q=0.8',\n",
    "            'Connection': 'keep-alive'}\n",
    "\n",
    "def get_query_results(query):\n",
    "    url = 'https://stackoverflow.com/search?q='+'+'.join(query)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    #print(r.status_code)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    x = parse_results(soup)\n",
    "    print('found {} results'.format(len(x)))\n",
    "    return x\n",
    "    \n",
    "def parse_results(soup):\n",
    "    d = {}\n",
    "    l =[]\n",
    "    total_results = soup.find(\"div\", class_=\"subheader results-header\").find('h2').get_text()\n",
    "    d['total_results'] = int(re.match('\\d*',total_results).group())\n",
    "    results = soup.find_all(\"div\", class_=\"question-summary search-result\")\n",
    "    i = 0\n",
    "    for result in results:\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            return l\n",
    "        votes = [v.get_text() for v in result.find_all(\"strong\")]\n",
    "        if len(votes) == 1:\n",
    "            votes.append(1)\n",
    "        link = result.find(\"div\", class_=\"result-link\").find('a')\n",
    "        query = link.attrs['title']\n",
    "        url = link.attrs['href']\n",
    "        votes.extend([query,url])\n",
    "        l.append(tuple(votes))\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef make_summaries(key, dic):\\n    d = defaultdict(dict)\\n    votes = 0\\n    answers = 0\\n    for vote,answer,query,_ in dic[list(original)[0]]:\\n        if query in shared:\\n            continue\\n            votes += int(vote)\\n            answers += int(answer)\\n        d[key]['votes'] = votes\\n        d[key]['answers'] = answers\\n        d[key]['unique'] = orig_unique\\n        d[key]['unique_perc'] = len(orig_unique)*1.0/len(orig_items)\\n        \""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" given dictionaries of suggested-queries' results and the original query results,\n",
    "    compares metrics of the returned queries\"\"\"\n",
    "def compare(suggestions, original):\n",
    "    # get result titles from original in a list\n",
    "    orig_items = [title for _,_,title,_ in original[list(original)[0]]]\n",
    "    summaries = defaultdict(dict)\n",
    "    orig_summaries = defaultdict(dict)\n",
    "    for k in suggestions:\n",
    "        # isolate titles for suggested results\n",
    "        sug_items = [title for _,_,title,_ in suggestions[k]]\n",
    "        # identify which results are shared with the original search results\n",
    "        shared = set(orig_items) & set(sug_items)\n",
    "        orig_unique = list(set(orig_items).difference(set(sug_items)))\n",
    "        sug_unique = list(set(sug_items).difference(set(orig_items)))\n",
    "        if len(sug_items) > 0:\n",
    "            concordance = len(shared)*1.0/len(sug_items)\n",
    "        else:\n",
    "            concordance = np.nan\n",
    "        \n",
    "        votes = 0\n",
    "        answers = 0\n",
    "        for vote,answer,query,_ in original[list(original)[0]]:\n",
    "            if query in shared:\n",
    "                continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "        orig_summaries[k]['unique'] = len(orig_unique)\n",
    "        try: \n",
    "            orig_summaries[k]['avg_unique_votes'] = votes*1.0/orig_summaries[k]['unique']\n",
    "            orig_summaries[k]['avg_unique_answers'] = answers*1.0/orig_summaries[k]['unique']\n",
    "        except ZeroDivisionError:\n",
    "            orig_summaries[k]['avg_unique_votes'] = 0\n",
    "            orig_summaries[k]['avg_unique_answers'] = 0\n",
    "        orig_summaries[k]['titles'] = orig_unique\n",
    "        orig_summaries[k]['concordance'] = concordance\n",
    "        \n",
    "        # calculate sum of votes and answers for all unique search results in suggestion\n",
    "        votes = 0\n",
    "        answers = 0\n",
    "        #distances = []\n",
    "        for vote,answer,query,url in suggestions[k]:\n",
    "            if query in shared:\n",
    "                continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "            #distances.append ()# cosine distance fucntion here)\n",
    "            # concordance\n",
    "        summaries[k]['unique'] = len(sug_unique)\n",
    "        summaries[k]['titles'] = sug_unique\n",
    "        summaries[k]['concordance'] = concordance\n",
    "        try: \n",
    "            summaries[k]['avg_unique_votes'] = votes*1.0/summaries[k]['unique']\n",
    "            summaries[k]['avg_unique_answers'] = answers*1.0/summaries[k]['unique']\n",
    "        except ZeroDivisionError:\n",
    "            summaries[k]['avg_unique_votes'] = 0\n",
    "            summaries[k]['avg_unique_answers'] = 0\n",
    "    return summaries, orig_summaries\n",
    "\n",
    "\"\"\"\n",
    "def make_summaries(key, dic):\n",
    "    d = defaultdict(dict)\n",
    "    votes = 0\n",
    "    answers = 0\n",
    "    for vote,answer,query,_ in dic[list(original)[0]]:\n",
    "        if query in shared:\n",
    "            continue\n",
    "            votes += int(vote)\n",
    "            answers += int(answer)\n",
    "        d[key]['votes'] = votes\n",
    "        d[key]['answers'] = answers\n",
    "        d[key]['unique'] = orig_unique\n",
    "        d[key]['unique_perc'] = len(orig_unique)*1.0/len(orig_items)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(queries, n_syns, topn, model, dic):\n",
    "    #results_dict = defaultdict(dict)\n",
    "    i = 0\n",
    "    for q in queries:\n",
    "        i+=1\n",
    "        print('starting query {}'.format(i))\n",
    "        dic[q] = evaluate_query(q, n_syns, topn, model)\n",
    "        if dic[q] == None:\n",
    "            print('query {} failed'.format(i))\n",
    "            continue\n",
    "        print('query {} completed'.format(i))\n",
    "    return dic\n",
    "        #results_dict[q] = evaluate_query(q, n_syns, topn, model)\n",
    "    #return results_dict\n",
    "\n",
    "def evaluate_query(orig_query, n_syns, topn, model):\n",
    "    # improtant local varaibles\n",
    "    orig_query_vec = clean(orig_query, return_tokens=True)\n",
    "\n",
    "    # generate, rank, filter suggestions\n",
    "    print('generating predictions')\n",
    "    preds = generate_alternatives(orig_query, n_syns, model)\n",
    "    best_preds = clean_preds(preds, topn)\n",
    "    best_preds2 = [(s,q) for s,q in best_preds if q!=' '.join(orig_query_vec)]\n",
    "    #orig = (model.score([orig_query_vec])[0], ' '.join(orig_query_vec))\n",
    "    \n",
    "    # query each suggestion and scrape relevent metrics\n",
    "    sug_q_results ={}\n",
    "    for _,q in best_preds2:\n",
    "        print('getting search results for \\\"{}\\\"'.format(q))\n",
    "        sug_q_results[q] = get_query_results(tokenizer.tokenize(q))\n",
    "        #print(sug_q_results[q])\n",
    "        #if sug_q_results[q] == []:\n",
    "        #    print('Website returned no results.')\n",
    "        #    return None\n",
    "        time.sleep(2.5)\n",
    "    \n",
    "    # get query results for original query\n",
    "    orig_q_results = {orig_query: get_query_results(orig_query_vec)}\n",
    "    \n",
    "    # get metrics\n",
    "    print('making comparisons')\n",
    "    summaries,orig_summaries = compare(sug_q_results, orig_q_results)\n",
    "    \n",
    "    \n",
    "    # calculate average metrics\n",
    "    avg_answers_dif_score = np.mean([summaries[k]['avg_unique_answers'] - \n",
    "                                  orig_summaries[k]['avg_unique_answers'] for k in summaries])\n",
    "    avg_votes_dif_score = np.mean([summaries[k]['avg_unique_votes'] - \n",
    "                                orig_summaries[k]['avg_unique_votes'] for k in summaries])\n",
    "    avg_unique_items = np.mean([summaries[k]['unique'] for k in summaries])\n",
    "\n",
    "    results = {}\n",
    "    orig_prob = model.score([orig_query_vec])[0]\n",
    "    results['avg_unique_items'] = avg_unique_items\n",
    "    results['avg_answers_dif_score'] = avg_answers_dif_score\n",
    "    results['avg_votes_dif_score'] = avg_votes_dif_score\n",
    "    results['avg_prob_dif_score'] = np.mean([np.exp(prob)-np.exp(orig_prob) for prob,q in best_preds2])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['initializing row pandas dataframe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting query 1\n",
      "generating predictions\n",
      "getting similar words\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'pandas' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-f70839b56830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-4fd314d889b3>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(queries, n_syns, topn, model, dic)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting query {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query {} failed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-4fd314d889b3>\u001b[0m in \u001b[0;36mevaluate_query\u001b[0;34m(orig_query, n_syns, topn, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# generate, rank, filter suggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generating predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_alternatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mbest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbest_preds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_preds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_query_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-df35f4969209>\u001b[0m in \u001b[0;36mgenerate_alternatives\u001b[0;34m(query, n, model, rmv_stop_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_alternatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmv_stop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'getting similar words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msyns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmv_stop_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# synonyms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'making combinations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcombs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_combinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# combinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-df35f4969209>\u001b[0m in \u001b[0;36mget_similar\u001b[0;34m(query, n, model, rmv_stop_words)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# for each word in original query, add topn similar words to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msyn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'pandas' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "r = defaultdict(dict)\n",
    "results = validate(queries, n_syns=5, topn=5, model=model, dic=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'initializing row pandas dataframe': {'avg_answers_dif_score': -0.41999999999999993,\n",
       "              'avg_prob_dif_score': 4.7898663e-09,\n",
       "              'avg_unique_items': 2.2000000000000002,\n",
       "              'avg_votes_dif_score': -1.8800000000000001}})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_answers_dif_score</th>\n",
       "      <th>avg_prob_dif_score</th>\n",
       "      <th>avg_unique_items</th>\n",
       "      <th>avg_votes_dif_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initializing row pandas dataframe</th>\n",
       "      <td>-0.42</td>\n",
       "      <td>4.789866e-09</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-1.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   avg_answers_dif_score  avg_prob_dif_score  \\\n",
       "initializing row pandas dataframe                  -0.42        4.789866e-09   \n",
       "\n",
       "                                   avg_unique_items  avg_votes_dif_score  \n",
       "initializing row pandas dataframe               2.2                -1.88  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting queries for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackapi import StackAPI\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection\n",
    "SITE = StackAPI('stackoverflow')\n",
    "SITE.page_size = 100\n",
    "SITE.max_pages = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attrib(posts, tag):\n",
    "    return [d[tag] for d in posts['items']]\n",
    "\n",
    "def append(filename, dat):    \n",
    "    with open(filename, 'a+') as f:\n",
    "        for item in dat:\n",
    "            f.write(item+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = SITE.fetch('questions', fromdate=datetime(2010,1,1), todate=datetime(2016,1,1),\n",
    "                                min=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Attributes: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tags',\n",
       " 'owner',\n",
       " 'is_answered',\n",
       " 'view_count',\n",
       " 'answer_count',\n",
       " 'score',\n",
       " 'last_activity_date',\n",
       " 'creation_date',\n",
       " 'last_edit_date',\n",
       " 'question_id',\n",
       " 'link',\n",
       " 'title']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Available Attributes: ')\n",
    "list(posts['items'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attrib(attrib):\n",
    "    return [x[attrib] for x in posts['items']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.048333333333332"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(get_attrib('score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[datetime.fromtimestamp(date) for date in get_attrib('creation_date')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - dates are randomly scattered between 2010 and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(path+'posts_for_validation.txt', 'w') as outfile:\n",
    "    json.dump(posts, outfile)\n",
    "\n",
    "with open(path+'titles_for_validation.txt', 'w') as f:\n",
    "    for title in get_attrib('title'):\n",
    "        f.write(title+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no need to use these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(iters, connection, typ, path,fname):\n",
    "    for i in range(iters):\n",
    "        posts = connection.fetch(typ, fromdate=datetime(2010,1,1), todate=datetime(2015,1,1),\n",
    "                                min=10, sort='votes')\n",
    "    \n",
    "        # parse and write titles\n",
    "        titles = get_attrib(posts, 'title')\n",
    "        append(path+fname, titles)\n",
    "\n",
    "        print('waiting {} seconds'.format(60+posts['backoff']))\n",
    "        time.sleep(60+posts['backoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/stevenfelix/Documents/DataScience_local/Insight/'\n",
    "get_posts(1, SITE, 'questions', path, 'recent_titles.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'titles_for_validation.txt', 'r') as f:\n",
    "    queries = f.readlines()\n",
    "\n",
    "queries =[html.unescape(q).strip() for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries100 = queries[0:100]\n",
    "len(queries100)\n",
    "## note, lots of special characters will get stripped in my pre-processor, \n",
    "## meaning that the original query will almost always perform better\n",
    "## need to retain proper names and commong things (C++, robots.txt, C#, function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "contractions = re.compile(r\"'|-|\\\"\")\n",
    "# all non alphanumeric\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "# single character removal\n",
    "singles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "# tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white spice\n",
    "# stop words\n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text, rmv_stop_words=False): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = symbols.sub(r' \\1 ', text)\n",
    "    text = singles.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    tokens = tokenizer.tokenize(text)     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "    return text,tokens\n",
    "\n",
    "# sentence splitter\n",
    "alteos = re.compile(r'([!\\?])')\n",
    "def sentences(l):\n",
    "    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n",
    "    return l.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install RoR on debian Squeeze\n",
      "install ror on debian squeeze\n",
      "['install', 'ror', 'debian', 'squeeze']\n",
      "\n",
      "\n",
      "CSS Line Height - bottom only\n",
      "css line height bottom only\n",
      "['css', 'line', 'height', 'bottom']\n",
      "\n",
      "\n",
      "Writing string to a file on a new line every time\n",
      "writing string to file on new line every time\n",
      "['writing', 'string', 'file', 'new', 'line', 'every', 'time']\n",
      "\n",
      "\n",
      "mysql error: Can't create a new thread (errno 11)\n",
      "mysql error cant create new thread errno 11 \n",
      "['mysql', 'error', 'cant', 'create', 'new', 'thread', 'errno', '11']\n",
      "\n",
      "\n",
      "CSS line-height alignment issues\n",
      "css lineheight alignment issues\n",
      "['css', 'lineheight', 'alignment', 'issues']\n",
      "\n",
      "\n",
      "Return 410 for all but robots.txt\n",
      "return 410 for all but robots txt\n",
      "['return', '410', 'robots', 'txt']\n",
      "\n",
      "\n",
      "Swift: Change a button color once pressed\n",
      "swift change button color once pressed\n",
      "['swift', 'change', 'button', 'color', 'pressed']\n",
      "\n",
      "\n",
      "Linked server can't start distributed transaction in trigger\n",
      "linked server cant start distributed transaction in trigger\n",
      "['linked', 'server', 'cant', 'start', 'distributed', 'transaction', 'trigger']\n",
      "\n",
      "\n",
      "lodash : how to loop with between a start value and end value\n",
      "lodash how to loop with between start value and end value\n",
      "['lodash', 'loop', 'start', 'value', 'end', 'value']\n",
      "\n",
      "\n",
      "Category implementation stuck in thread with increasing memory heap space\n",
      "category implementation stuck in thread with increasing memory heap space\n",
      "['category', 'implementation', 'stuck', 'thread', 'increasing', 'memory', 'heap', 'space']\n",
      "\n",
      "\n",
      "TestNG tests don't run in Maven Project alongside JUnit4 projects\n",
      "testng tests dont run in maven project alongside junit4 projects\n",
      "['testng', 'tests', 'dont', 'run', 'maven', 'project', 'alongside', 'junit4', 'projects']\n",
      "\n",
      "\n",
      "Unable to initialize MockHttpServletRequest in my unit tests\n",
      "unable to initialize mockhttpservletrequest in my unit tests\n",
      "['unable', 'initialize', 'mockhttpservletrequest', 'unit', 'tests']\n",
      "\n",
      "\n",
      "How to add a border just on the top side of a UIView\n",
      "how to add border just on the top side of uiview\n",
      "['add', 'border', 'top', 'side', 'uiview']\n",
      "\n",
      "\n",
      "Android Options Menu in Fragment\n",
      "android options menu in fragment\n",
      "['android', 'options', 'menu', 'fragment']\n",
      "\n",
      "\n",
      "How do I get a white circular progress bar in Android 2.2 and above?\n",
      "how do get white circular progress bar in android . and above \n",
      "['get', 'white', 'circular', 'progress', 'bar', 'android']\n",
      "\n",
      "\n",
      "How do I change default message for permissions for Location manager in iOS 4?\n",
      "how do change default message for permissions for location manager in ios ? \n",
      "['change', 'default', 'message', 'permissions', 'location', 'manager', 'ios']\n",
      "\n",
      "\n",
      "How to add sort by \"Best Seller\" in tool bar in product listing page?\n",
      "how to add sort by best seller in tool bar in product listing page \n",
      "['add', 'sort', 'best', 'seller', 'tool', 'bar', 'product', 'listing', 'page']\n",
      "\n",
      "\n",
      "Error creating Runnable jar in eclipse\n",
      "error creating runnable jar in eclipse\n",
      "['error', 'creating', 'runnable', 'jar', 'eclipse']\n",
      "\n",
      "\n",
      "Android \"achartengine\" points on Y-axis\n",
      "android achartengine points on yaxis\n",
      "['android', 'achartengine', 'points', 'yaxis']\n",
      "\n",
      "\n",
      "SQL Server stops loading assembly\n",
      "sql server stops loading assembly\n",
      "['sql', 'server', 'stops', 'loading', 'assembly']\n",
      "\n",
      "\n",
      "God cannot start redis server. Getting this error: `/var/run/redis/redis-server.pid': Permission denied\n",
      "god cannot start redis server getting this error `/ var run redis redisserver pid permission denied\n",
      "['god', 'cannot', 'start', 'redis', 'server', 'getting', 'error', 'var', 'run', 'redis', 'redisserver', 'pid', 'permission', 'denied']\n",
      "\n",
      "\n",
      "How to create dynamic directory structure on SD card in Android?\n",
      "how to create dynamic directory structure on sd card in android \n",
      "['create', 'dynamic', 'directory', 'structure', 'sd', 'card', 'android']\n",
      "\n",
      "\n",
      "Will an app developed for S60 3rd edition, work on Symbian belle?\n",
      "will an app developed for s60 3rd edition work on symbian belle \n",
      "['app', 'developed', 's60', '3rd', 'edition', 'work', 'symbian', 'belle']\n",
      "\n",
      "\n",
      "Sheets are not displaying properly on OSX\n",
      "sheets are not displaying properly on osx\n",
      "['sheets', 'displaying', 'properly', 'osx']\n",
      "\n",
      "\n",
      "SecKeychainFindGenericPassword not working for osx\n",
      "seckeychainfindgenericpassword not working for osx\n",
      "['seckeychainfindgenericpassword', 'working', 'osx']\n",
      "\n",
      "\n",
      "How to scan for available bluetooth devices in range in android?\n",
      "how to scan for available bluetooth devices in range in android \n",
      "['scan', 'available', 'bluetooth', 'devices', 'range', 'android']\n",
      "\n",
      "\n",
      "Convert JSON object with duplicate keys to JSON array\n",
      "convert json object with duplicate keys to json array\n",
      "['convert', 'json', 'object', 'duplicate', 'keys', 'json', 'array']\n",
      "\n",
      "\n",
      "What's a good hash function for English words?\n",
      "whats good hash function for english words \n",
      "['whats', 'good', 'hash', 'function', 'english', 'words']\n",
      "\n",
      "\n",
      "Tint menu icons\n",
      "tint menu icons\n",
      "['tint', 'menu', 'icons']\n",
      "\n",
      "\n",
      "getting additional data in passwordData when using SecKeychainFindGenericPassword in osx\n",
      "getting additional data in passworddata when using seckeychainfindgenericpassword in osx\n",
      "['getting', 'additional', 'data', 'passworddata', 'using', 'seckeychainfindgenericpassword', 'osx']\n",
      "\n",
      "\n",
      "order of calling constructors in Python\n",
      "order of calling constructors in python\n",
      "['order', 'calling', 'constructors', 'python']\n",
      "\n",
      "\n",
      "How can I access the table comment from a mysql table?\n",
      "how can access the table comment from mysql table \n",
      "['access', 'table', 'comment', 'mysql', 'table']\n",
      "\n",
      "\n",
      "CheckBox Preference Activity how to start and stop service from Intent.ACTION_BOOT_COMPLETED using broadcast receiver\n",
      "checkbox preference activity how to start and stop service from intent action_boot_completed using broadcast receiver\n",
      "['checkbox', 'preference', 'activity', 'start', 'stop', 'service', 'intent', 'action_boot_completed', 'using', 'broadcast', 'receiver']\n",
      "\n",
      "\n",
      "Twitter bootstrap modal-backdrop doesn't disappear\n",
      "twitter bootstrap modalbackdrop doesnt disappear\n",
      "['twitter', 'bootstrap', 'modalbackdrop', 'doesnt', 'disappear']\n",
      "\n",
      "\n",
      "How to make rotating menu\n",
      "how to make rotating menu\n",
      "['make', 'rotating', 'menu']\n",
      "\n",
      "\n",
      "Number of resque workers\n",
      "number of resque workers\n",
      "['number', 'resque', 'workers']\n",
      "\n",
      "\n",
      "How to ignore ansible SSH authenticity checking?\n",
      "how to ignore ansible ssh authenticity checking \n",
      "['ignore', 'ansible', 'ssh', 'authenticity', 'checking']\n",
      "\n",
      "\n",
      "how to create custom spinner like border around the spinner with down triangle on the right side?\n",
      "how to create custom spinner like border around the spinner with down triangle on the right side \n",
      "['create', 'custom', 'spinner', 'like', 'border', 'around', 'spinner', 'triangle', 'right', 'side']\n",
      "\n",
      "\n",
      "Docker and symfony\n",
      "docker and symfony\n",
      "['docker', 'symfony']\n",
      "\n",
      "\n",
      "Can't align two html5 input range\n",
      "cant align two html5 input range\n",
      "['cant', 'align', 'two', 'html5', 'input', 'range']\n",
      "\n",
      "\n",
      "Regex capture every occurrence of a word within two delimiters\n",
      "regex capture every occurrence of word within two delimiters\n",
      "['regex', 'capture', 'every', 'occurrence', 'word', 'within', 'two', 'delimiters']\n",
      "\n",
      "\n",
      "com.sun.jdi.InvocationException occurred invoking method\n",
      "com sun jdi invocationexception occurred invoking method\n",
      "['com', 'sun', 'jdi', 'invocationexception', 'occurred', 'invoking', 'method']\n",
      "\n",
      "\n",
      "container_of (inode->i_cdev, struct scull_dev, cdev) explanation\n",
      "container_of inode i_cdev struct scull_dev cdev explanation\n",
      "['container_of', 'inode', 'i_cdev', 'struct', 'scull_dev', 'cdev', 'explanation']\n",
      "\n",
      "\n",
      "Generate self signed certificate on the fly\n",
      "generate self signed certificate on the fly\n",
      "['generate', 'self', 'signed', 'certificate', 'fly']\n",
      "\n",
      "\n",
      "Extension Method vs. Helper Class\n",
      "extension method vs helper class\n",
      "['extension', 'method', 'vs', 'helper', 'class']\n",
      "\n",
      "\n",
      "C# How to redirect stream to the console Out?\n",
      "c how to redirect stream to the console out \n",
      "['c', 'redirect', 'stream', 'console']\n",
      "\n",
      "\n",
      "Friend-of-a-friend (FOAF) support in Facebook\n",
      "friendofafriend foaf support in facebook\n",
      "['friendofafriend', 'foaf', 'support', 'facebook']\n",
      "\n",
      "\n",
      "Edit control capture enter key\n",
      "edit control capture enter key\n",
      "['edit', 'control', 'capture', 'enter', 'key']\n",
      "\n",
      "\n",
      "How to create a painting application using PHP and HTML 5?\n",
      "how to create painting application using php and html ? \n",
      "['create', 'painting', 'application', 'using', 'php', 'html']\n",
      "\n",
      "\n",
      "Pandas groupby cumulative sum\n",
      "pandas groupby cumulative sum\n",
      "['pandas', 'groupby', 'cumulative', 'sum']\n",
      "\n",
      "\n",
      "Fire \"afteredit\" after edit the entire row in ExtJS grid?\n",
      "fire afteredit after edit the entire row in extjs grid \n",
      "['fire', 'afteredit', 'edit', 'entire', 'row', 'extjs', 'grid']\n",
      "\n",
      "\n",
      "Android:\"Unexpected end of stream\" exception downloading large files\n",
      "android unexpected end of stream exception downloading large files\n",
      "['android', 'unexpected', 'end', 'stream', 'exception', 'downloading', 'large', 'files']\n",
      "\n",
      "\n",
      "SELECT FROM OPENROWSET( BULK...) changing special characters\n",
      "select from openrowset bulk ...) changing special characters\n",
      "['select', 'openrowset', 'bulk', 'changing', 'special', 'characters']\n",
      "\n",
      "\n",
      "How to insert blank pages at the beginning or at the end of a document without altering page number in LibreOffice?\n",
      "how to insert blank pages at the beginning or at the end of document without altering page number in libreoffice \n",
      "['insert', 'blank', 'pages', 'beginning', 'end', 'document', 'without', 'altering', 'page', 'number', 'libreoffice']\n",
      "\n",
      "\n",
      "how to solve \"unzip: cannot find or open\" error in linux OS\n",
      "how to solve unzip cannot find or open error in linux os\n",
      "['solve', 'unzip', 'cannot', 'find', 'open', 'error', 'linux', 'os']\n",
      "\n",
      "\n",
      "Insert picture to Excel VBA, method Pictures failed\n",
      "insert picture to excel vba method pictures failed\n",
      "['insert', 'picture', 'excel', 'vba', 'method', 'pictures', 'failed']\n",
      "\n",
      "\n",
      "multiple keys keydown on keyboard at once time in c#\n",
      "multiple keys keydown on keyboard at once time in # \n",
      "['multiple', 'keys', 'keydown', 'keyboard', 'time']\n",
      "\n",
      "\n",
      "Java 8 Lambdas - equivalent of c# OfType\n",
      "java lambdas equivalent of # oftype\n",
      "['java', 'lambdas', 'equivalent', 'oftype']\n",
      "\n",
      "\n",
      "calculate age in years months and days\n",
      "calculate age in years months and days\n",
      "['calculate', 'age', 'years', 'months', 'days']\n",
      "\n",
      "\n",
      "How to define sealed class in C++?\n",
      "how to define sealed class in ++? \n",
      "['define', 'sealed', 'class']\n",
      "\n",
      "\n",
      "Which browsers still support CSS expressions\n",
      "which browsers still support css expressions\n",
      "['browsers', 'still', 'support', 'css', 'expressions']\n",
      "\n",
      "\n",
      "How do I convert an existing callback API to promises?\n",
      "how do convert an existing callback api to promises \n",
      "['convert', 'existing', 'callback', 'api', 'promises']\n",
      "\n",
      "\n",
      "How to find current zoom level in a Google Map?\n",
      "how to find current zoom level in google map \n",
      "['find', 'current', 'zoom', 'level', 'google', 'map']\n",
      "\n",
      "\n",
      "Print Text in dlib window\n",
      "print text in dlib window\n",
      "['print', 'text', 'dlib', 'window']\n",
      "\n",
      "\n",
      "NPM doesn't install module dependencies\n",
      "npm doesnt install module dependencies\n",
      "['npm', 'doesnt', 'install', 'module', 'dependencies']\n",
      "\n",
      "\n",
      "NodeJS NPM Proxy error when installing grunt\n",
      "nodejs npm proxy error when installing grunt\n",
      "['nodejs', 'npm', 'proxy', 'error', 'installing', 'grunt']\n",
      "\n",
      "\n",
      "Using n github repositories for a single github-page website?\n",
      "using github repositories for single githubpage website \n",
      "['using', 'github', 'repositories', 'single', 'githubpage', 'website']\n",
      "\n",
      "\n",
      "Should there be one controller per view in Angularjs?\n",
      "should there be one controller per view in angularjs \n",
      "['one', 'controller', 'per', 'view', 'angularjs']\n",
      "\n",
      "\n",
      "Calculate Age in MySQL (InnoDb)\n",
      "calculate age in mysql innodb \n",
      "['calculate', 'age', 'mysql', 'innodb']\n",
      "\n",
      "\n",
      "Unable to get GoogleMap OnMarkerClickListener to work\n",
      "unable to get googlemap onmarkerclicklistener to work\n",
      "['unable', 'get', 'googlemap', 'onmarkerclicklistener', 'work']\n",
      "\n",
      "\n",
      "npm not working - \"read ECONNRESET\"\n",
      "npm not working read econnreset\n",
      "['npm', 'working', 'read', 'econnreset']\n",
      "\n",
      "\n",
      "How to access/ explore remote(server) files using Filesystem (HTML5)?\n",
      "how to access explore remote server files using filesystem html5 )? \n",
      "['access', 'explore', 'remote', 'server', 'files', 'using', 'filesystem', 'html5']\n",
      "\n",
      "\n",
      "How to reset google oauth 2.0 authorization?\n",
      "how to reset google oauth . authorization \n",
      "['reset', 'google', 'oauth', 'authorization']\n",
      "\n",
      "\n",
      "How to use ? : if statements with Razor and inline code blocks\n",
      "how to use : if statements with razor and inline code blocks\n",
      "['use', 'statements', 'razor', 'inline', 'code', 'blocks']\n",
      "\n",
      "\n",
      "Android: How can we hide/show floating action button(fab) in xml layout\n",
      "android how can we hide show floating action button fab in xml layout\n",
      "['android', 'hide', 'show', 'floating', 'action', 'button', 'fab', 'xml', 'layout']\n",
      "\n",
      "\n",
      "Why drawcontours in OpenCV doesnt fill contours in the edge of the image?\n",
      "why drawcontours in opencv doesn t fill contours in the edge of the image \n",
      "['drawcontours', 'opencv', 'fill', 'contours', 'edge', 'image']\n",
      "\n",
      "\n",
      "onConfigurationChanged not getting called\n",
      "onconfigurationchanged not getting called\n",
      "['onconfigurationchanged', 'getting', 'called']\n",
      "\n",
      "\n",
      "java.lang.ClassNotFoundException: org.apache.poi.xssf.usermodel.XSSFWorkbook\n",
      "java lang classnotfoundexception org apache poi xssf usermodel xssfworkbook\n",
      "['java', 'lang', 'classnotfoundexception', 'org', 'apache', 'poi', 'xssf', 'usermodel', 'xssfworkbook']\n",
      "\n",
      "\n",
      "Error: Cannot find module 'gulp-sass'\n",
      "error cannot find module gulpsass\n",
      "['error', 'cannot', 'find', 'module', 'gulpsass']\n",
      "\n",
      "\n",
      "Getting custom assembly attributes without loading into current AppDomain\n",
      "getting custom assembly attributes without loading into current appdomain\n",
      "['getting', 'custom', 'assembly', 'attributes', 'without', 'loading', 'current', 'appdomain']\n",
      "\n",
      "\n",
      "Limit amount of links shown with Laravel pagination\n",
      "limit amount of links shown with laravel pagination\n",
      "['limit', 'amount', 'links', 'shown', 'laravel', 'pagination']\n",
      "\n",
      "\n",
      "Exclude null values using JSONBuilder in Groovy\n",
      "exclude null values using jsonbuilder in groovy\n",
      "['exclude', 'null', 'values', 'using', 'jsonbuilder', 'groovy']\n",
      "\n",
      "\n",
      "Finding matching objects in an array of objects?\n",
      "finding matching objects in an array of objects \n",
      "['finding', 'matching', 'objects', 'array', 'objects']\n",
      "\n",
      "\n",
      "How can I open a .tex file?\n",
      "how can open tex file \n",
      "['open', 'tex', 'file']\n",
      "\n",
      "\n",
      "Roles vs Claims Authorization Asp.net web api-2 with WIF and OWIN Middleware\n",
      "roles vs claims authorization asp net web api2 with wif and owin middleware\n",
      "['roles', 'vs', 'claims', 'authorization', 'asp', 'net', 'web', 'api2', 'wif', 'owin', 'middleware']\n",
      "\n",
      "\n",
      "How to replace a @JoinColumn with a hardcoded value?\n",
      "how to replace joincolumn with hardcoded value \n",
      "['replace', 'joincolumn', 'hardcoded', 'value']\n",
      "\n",
      "\n",
      "JavaScript Extending Class\n",
      "javascript extending class\n",
      "['javascript', 'extending', 'class']\n",
      "\n",
      "\n",
      "annotation to filter results of a @OneToMany association\n",
      "annotation to filter results of onetomany association\n",
      "['annotation', 'filter', 'results', 'onetomany', 'association']\n",
      "\n",
      "\n",
      "sklearn selectKbest: which variables were chosen?\n",
      "sklearn selectkbest which variables were chosen \n",
      "['sklearn', 'selectkbest', 'variables', 'chosen']\n",
      "\n",
      "\n",
      "TinyMCE opened in jqueryUI modal dialog\n",
      "tinymce opened in jqueryui modal dialog\n",
      "['tinymce', 'opened', 'jqueryui', 'modal', 'dialog']\n",
      "\n",
      "\n",
      "3 images in a row centered in container\n",
      "3 images in row centered in container\n",
      "['3', 'images', 'row', 'centered', 'container']\n",
      "\n",
      "\n",
      "How do I find objects with a property inside another object in JavaScript\n",
      "how do find objects with property inside another object in javascript\n",
      "['find', 'objects', 'property', 'inside', 'another', 'object', 'javascript']\n",
      "\n",
      "\n",
      "Apache MINA reading from IoSession\n",
      "apache mina reading from iosession\n",
      "['apache', 'mina', 'reading', 'iosession']\n",
      "\n",
      "\n",
      "What is the Tibco stack\n",
      "what is the tibco stack\n",
      "['tibco', 'stack']\n",
      "\n",
      "\n",
      "Creating New Roles and Permissions Dynamically in Spring Security 3\n",
      "creating new roles and permissions dynamically in spring security 3\n",
      "['creating', 'new', 'roles', 'permissions', 'dynamically', 'spring', 'security', '3']\n",
      "\n",
      "\n",
      "Rails json object with model associations\n",
      "rails json object with model associations\n",
      "['rails', 'json', 'object', 'model', 'associations']\n",
      "\n",
      "\n",
      "Proper Proguard configuration to keep static inner class\n",
      "proper proguard configuration to keep static inner class\n",
      "['proper', 'proguard', 'configuration', 'keep', 'static', 'inner', 'class']\n",
      "\n",
      "\n",
      "how do i know if nodetool repair is finished\n",
      "how do know if nodetool repair is finished\n",
      "['know', 'nodetool', 'repair', 'finished']\n",
      "\n",
      "\n",
      "How do I stop tracking in Vuforia for Unity\n",
      "how do stop tracking in vuforia for unity\n",
      "['stop', 'tracking', 'vuforia', 'unity']\n",
      "\n",
      "\n",
      "Returning a QueryDSL BooleanExpression that evaluates to true\n",
      "returning querydsl booleanexpression that evaluates to true\n",
      "['returning', 'querydsl', 'booleanexpression', 'evaluates', 'true']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in queries100:\n",
    "    print(q)\n",
    "    tex,tok = clean(q, rmv_stop_words=True)\n",
    "    print(tex)\n",
    "    print(tok)\n",
    "    print('\\n')\n",
    "# do remove stop words\n",
    "# don't remove trailing s !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_clean_7 = []\n",
    "for q in queries100:\n",
    "    tok = clean(q, rmv_stop_words=True, return_tokens=True)\n",
    "    if len(tok) > 7:\n",
    "        continue\n",
    "    queries_clean_7.append(' '.join(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['install ror debian squeeze',\n",
       " 'css line height bottom',\n",
       " 'writing string file new line every time',\n",
       " 'css lineheight alignment issues',\n",
       " 'return 410 robots txt',\n",
       " 'swift change button color pressed',\n",
       " 'linked server cant start distributed transaction trigger',\n",
       " 'lodash loop start value end value',\n",
       " 'unable initialize mockhttpservletrequest unit tests',\n",
       " 'add border top side uiview']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(queries_clean_7))\n",
    "queries_clean_7[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting query 1\n",
      "generating predictions\n",
      "getting similar words\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'install ror debian squeeze' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-0aad6e1d0a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_clean_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-2f75dab0221c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(queries, n_syns, topn, model, dic)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting query {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'query {} failed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-2f75dab0221c>\u001b[0m in \u001b[0;36mevaluate_query\u001b[0;34m(orig_query, n_syns, topn, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# generate, rank, filter suggestions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generating predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_alternatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_syns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mbest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbest_preds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_preds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_query_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-250ea2ce6e59>\u001b[0m in \u001b[0;36mgenerate_alternatives\u001b[0;34m(query, n, model, rmv_stop_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_alternatives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmv_stop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'getting similar words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msyns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmv_stop_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# synonyms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'making combinations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcombs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_combinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# combinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-250ea2ce6e59>\u001b[0m in \u001b[0;36mget_similar\u001b[0;34m(query, n, model, rmv_stop_words)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# for each word in original query, add topn similar words to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msyn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \"\"\"\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'install ror debian squeeze' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "r = defaultdict(dict)\n",
    "results = validate(queries_clean_7, n_syns=3, topn=5, model=model, dic=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_answers_dif_score</th>\n",
       "      <th>avg_prob_dif_score</th>\n",
       "      <th>avg_unique_items</th>\n",
       "      <th>avg_votes_dif_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iterating over pandas dataframe</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-25.825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 avg_answers_dif_score  avg_prob_dif_score  \\\n",
       "iterating over pandas dataframe                  0.125            0.000004   \n",
       "\n",
       "                                 avg_unique_items  avg_votes_dif_score  \n",
       "iterating over pandas dataframe               8.0              -25.825  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
