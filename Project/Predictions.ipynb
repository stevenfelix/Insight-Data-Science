{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "#import string\n",
    "from nltk.tokenize import RegexpTokenizer # tokenizing\n",
    "from nltk.corpus import stopwords  # list of stop words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # lemmatizer\n",
    "\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 09:42:41,322 : INFO : loading Word2Vec object from /Users/stevenfelix/Documents/DataScience_local/Insight/model_full_30M_sg_200_5_2\n",
      "2018-01-17 09:42:42,091 : INFO : loading wv recursively from /Users/stevenfelix/Documents/DataScience_local/Insight/model_full_30M_sg_200_5_2.wv.* with mmap=None\n",
      "2018-01-17 09:42:42,091 : INFO : loading syn0 from /Users/stevenfelix/Documents/DataScience_local/Insight/model_full_30M_sg_200_5_2.wv.syn0.npy with mmap=None\n",
      "2018-01-17 09:42:42,231 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-01-17 09:42:42,232 : INFO : loading syn1neg from /Users/stevenfelix/Documents/DataScience_local/Insight/model_full_30M_sg_200_5_2.syn1neg.npy with mmap=None\n",
      "2018-01-17 09:42:42,365 : INFO : setting ignored attribute cum_table to None\n",
      "2018-01-17 09:42:42,365 : INFO : loaded /Users/stevenfelix/Documents/DataScience_local/Insight/model_full_30M_sg_200_5_2\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/stevenfelix/Documents/DataScience_local/Insight/\"\n",
    "file = 'model_full_30M_sg_200_5_2'\n",
    "model_full = gensim.models.word2vec.Word2Vec.load(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 0.00012694114),\n",
       " ('all', 0.00011346572),\n",
       " ('row', 0.0001107741),\n",
       " ('one', 0.00010965685),\n",
       " ('element', 0.00010931836),\n",
       " ('column', 0.00010459485),\n",
       " ('match', 0.00010246703),\n",
       " ('list', 9.8215613e-05),\n",
       " ('number', 9.4541174e-05),\n",
       " ('table', 9.423896e-05)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"How to find all questions that were duplicates of another question\".split()\n",
    "model_full.predict_output_word(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries=['python binomial regression', 'dropping duplicates by index in pandas dataframe',\n",
    "             'latent community topic analysis', 'statsmodels mixed effects']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+') # tokens separated by white spice\n",
    "stops = set(stopwords.words('english')) # list of english stop words\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(title, rmv_stop_words=False):\n",
    "    tokens = tokenizer.tokenize(title.lower())     # tokenize\n",
    "    if rmv_stop_words:\n",
    "        tokens = [i for i in tokens if not i in stops] # remove stop words\n",
    "    normalized = [lemma.lemmatize(token) for token in tokens] # lemma\n",
    "    return normalized\n",
    "\n",
    "def predict(query):\n",
    "    q = clean(query, rmv_stop_words=True)\n",
    "    print('Original query: {}'.format(query))\n",
    "    #print(q)\n",
    "    print('\\nTry these related searches:')\n",
    "    for word in q:\n",
    "        missing = q[:]\n",
    "        ind = q.index(word)\n",
    "        missing.remove(word)\n",
    "        print('\\n')\n",
    "        for syn in model_full.most_similar([word],topn=5):\n",
    "            full = missing[:]\n",
    "            full.insert(ind,syn[0])\n",
    "            print(' '.join(full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-17 09:42:48,450 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: loop over pandas dataframe\n",
      "\n",
      "Try these related searches:\n",
      "\n",
      "\n",
      "looping panda dataframe\n",
      "iteration panda dataframe\n",
      "foreach panda dataframe\n",
      "forloop panda dataframe\n",
      "iterates panda dataframe\n",
      "\n",
      "\n",
      "loop dataframe dataframe\n",
      "loop multiindex dataframe\n",
      "loop dataframes dataframe\n",
      "loop datetimeindex dataframe\n",
      "loop read_csv dataframe\n",
      "\n",
      "\n",
      "loop panda panda\n",
      "loop panda dataframes\n",
      "loop panda multiindex\n",
      "loop panda datetimeindex\n",
      "loop panda cbind\n"
     ]
    }
   ],
   "source": [
    "predict(\"loop over pandas dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
